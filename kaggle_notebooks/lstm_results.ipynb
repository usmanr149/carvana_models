{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/carvana-image-masking-challenge/train_hq.zip\n/kaggle/input/carvana-image-masking-challenge/train_masks.zip\n/kaggle/input/carvana-image-masking-challenge/train.zip\n/kaggle/input/carvana-image-masking-challenge/29bb3ece3180_11.jpg\n/kaggle/input/carvana-image-masking-challenge/train_masks.csv.zip\n/kaggle/input/carvana-image-masking-challenge/test_hq.zip\n/kaggle/input/carvana-image-masking-challenge/sample_submission.csv.zip\n/kaggle/input/carvana-image-masking-challenge/metadata.csv.zip\n/kaggle/input/carvana-image-masking-challenge/test.zip\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\nfrom skimage.util import random_noise\nimport matplotlib.pylab as plt","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/carvana-image-masking-challenge/'\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(DATA_PATH + \"train_masks.csv.zip\",\"r\") as z:\n    z.extractall(\".\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(DATA_PATH + \"train.zip\",\"r\") as z:\n    z.extractall(\".\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(DATA_PATH + \"train_masks.zip\",\"r\") as z:\n    z.extractall(\".\")","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nfrom glob import glob\n\nINPUT_PATH = '.'\nDATA_PATH = INPUT_PATH\nTRAIN_DATA = os.path.join(DATA_PATH, \"train\")\nTRAIN_MASKS_DATA = os.path.join(DATA_PATH, \"train_masks\")","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files = np.array(glob(os.path.join(TRAIN_DATA, \"*.jpg\")))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids_ = np.array([s[len(TRAIN_DATA)+1:-6] for s in train_files])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids_ = np.unique(train_ids_)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_ids_)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"318"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids = train_ids_[:250]\ntest_ids = train_ids_[250:]","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image_size = (864//2,576//2)\nimage_size = (128,128)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_flip(img, mask, u=0.5):\n    if np.random.random() < u:\n        img = flip_axis(img, 1)\n        mask = flip_axis(mask, 1)\n    return img, mask\n\ndef rotate(x, theta, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest', cval=0.):\n    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n                                [np.sin(theta), np.cos(theta), 0],\n                                [0, 0, 1]])\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    transform_matrix = img_gen.transform_matrix_offset_center(rotation_matrix, h, w)\n    x = img_gen.apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\ndef random_rotate(img, mask, rotate_limit=(-20, 20), u=0.5):\n    if np.random.random() < u:\n        theta = np.pi / 180 * np.random.uniform(rotate_limit[0], rotate_limit[1])\n        img = ndimage.rotate(img, theta, reshape=False)\n        mask = ndimage.rotate(mask, theta, reshape=False)\n\n#         img = rotate(img, theta)\n#         mask = rotate(mask, theta)\n    return img, mask\n\ndef random_shift(img, mask, w_limit=(-0.1, 0.1), h_limit=(-0.1, 0.1), u=0.5):\n    if np.random.random() < u:\n        wshift = np.random.uniform(w_limit[0], w_limit[1])\n        hshift = np.random.uniform(h_limit[0], h_limit[1])\n                \n        img = ndimage.shift(img, (wshift, hshift, 0))\n        mask = ndimage.shift(mask, (wshift, hshift))\n#         img = shift(img, wshift, hshift)\n#         mask = shift(mask, wshift, hshift)\n    return img, mask\n\ndef shift(x, wshift, hshift, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest', cval=0.):\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    tx = hshift * h\n    ty = wshift * w\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    transform_matrix = translation_matrix  # no need to do offset\n    x = img_gen.apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\ndef zoom(x, zx, zy, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest', cval=0.):\n    zoom_matrix = np.array([[zx, 0, 0],\n                            [0, zy, 0],\n                            [0, 0, 1]])\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    transform_matrix = img_gen.transform_matrix_offset_center(zoom_matrix, h, w)\n    x = img_gen.apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\ndef random_zoom(img, mask, zoom_range=(0.8, 1), u=0.5):\n    if np.random.random() < u:\n        zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n        img = zoom(img, zx, zy)\n        mask = zoom(mask, zx, zy)\n    return img, mask\n\ndef shear(x, shear, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest', cval=0.):\n    shear_matrix = np.array([[1, -np.sin(shear), 0],\n                             [0, np.cos(shear), 0],\n                             [0, 0, 1]])\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    transform_matrix = img_gen.transform_matrix_offset_center(shear_matrix, h, w)\n    x = img_gen.apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\ndef random_shear(img, mask, intensity_range=(-0.5, 0.5), u=0.5):\n    if np.random.random() < u:\n        sh = np.random.uniform(-intensity_range[0], intensity_range[1])\n        img = shear(img, sh)\n        mask = shear(mask, sh)\n    return img, mask\n\ndef random_channel_shift(x, limit, channel_axis=2):\n    x = np.rollaxis(x, channel_axis, 0)\n    min_x, max_x = np.min(x), np.max(x)\n    channel_images = [np.clip(x_ch + np.random.uniform(-limit, limit), min_x, max_x) for x_ch in x]\n    x = np.stack(channel_images, axis=0)\n    x = np.rollaxis(x, 0, channel_axis + 1)\n    return x\n\ndef random_gray(img, u=0.5):\n    if np.random.random() < u:\n        coef = np.array([[[0.114, 0.587, 0.299]]])  # rgb to gray (YCbCr)\n        gray = np.sum(img * coef, axis=2)\n        img = np.dstack((gray, gray, gray))\n    return img\n\ndef random_contrast(img, limit=(-0.3, 0.3), u=0.5):\n    if np.random.random() < u:\n        alpha = 1.0 + np.random.uniform(limit[0], limit[1])\n        coef = np.array([[[0.114, 0.587, 0.299]]])  # rgb to gray (YCbCr)\n        gray = img * coef\n        gray = (3.0 * (1.0 - alpha) / gray.size) * np.sum(gray)\n        img = alpha * img + gray\n        img = np.clip(img, 0., 1.)\n    return img\n\ndef random_brightness(img, limit=(-0.3, 0.3), u=0.5):\n    if np.random.random() < u:\n        alpha = 1.0 + np.random.uniform(limit[0], limit[1])\n        img = alpha * img\n        img = np.clip(img, 0., 1.)\n    return img\n\ndef random_saturation(img, limit=(-0.3, 0.3), u=0.5):\n    if np.random.random() < u:\n        alpha = 1.0 + np.random.uniform(limit[0], limit[1])\n        coef = np.array([[[0.114, 0.587, 0.299]]])\n        gray = img * coef\n        gray = np.sum(gray, axis=2, keepdims=True)\n        img = alpha * img + (1. - alpha) * gray\n        img = np.clip(img, 0., 1.)\n    return img\n\ndef random_augmentation(img, mask):\n    img = random_channel_shift(img, limit=0.05)\n    img = random_brightness(img, (-0.5, 0.5), 0.5)\n    img = random_contrast(img, (-0.5, 0.5), 0.5)\n    img = random_saturation(img, (-0.5, 0.5), 0.5)\n    img = random_gray(img, 0.2)\n    img, mask = random_rotate(img, mask, (-20, 20), 0.5)\n#     img, mask = random_shear(img, mask, (-0.3, 0.3), 0.2)\n    img, mask = random_flip(img, mask, 0.3)\n    img, mask = random_shift(img, mask, (-20, 20), (-20, 20), 0.3)\n#     img, mask = random_zoom(img, mask, (0.8, 1), 0.3)\n    return img, mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_filename(image_id, image_type):\n    check_dir = False\n    if \"Train\" == image_type:\n        ext = 'jpg'\n        data_path = TRAIN_DATA\n        suffix = ''\n    elif \"Train_mask\" in image_type:\n        ext = 'gif'\n        data_path = TRAIN_MASKS_DATA\n        suffix = '_mask'\n    elif \"Test\" in image_type:\n        ext = 'jpg'\n        data_path = TEST_DATA\n        suffix = ''\n    else:\n        raise Exception(\"Image type '%s' is not recognized\" % image_type)\n\n    if check_dir and not os.path.exists(data_path):\n        os.makedirs(data_path)\n\n    return os.path.join(data_path, \"{}{}.{}\".format(image_id, suffix, ext))\n\nimport cv2\nfrom PIL import Image\n\ndef mean_shift(input_img):\n    # Shape of original image\n    originShape = input_img.shape\n    # Converting image into array of dimension [nb of pixels in originImage, 3]\n    # based on r g b intensities    \n    flatImg=np.reshape(input_img, [-1, 3])\n\n\n    # Estimate bandwidth for meanshift algorithm    \n    bandwidth = estimate_bandwidth(flatImg, quantile=0.1, n_samples=50)    \n    ms = MeanShift(bandwidth = bandwidth, bin_seeding=True)\n\n    # Performing meanshift on flatImg    \n    ms.fit(flatImg)\n\n    # (r,g,b) vectors corresponding to the different clusters after meanshift    \n    labels=ms.labels_\n    \n    # Remaining colors after meanshift    \n    cluster_centers = ms.cluster_centers_    \n\n    # Finding and diplaying the number of clusters    \n    labels_unique = np.unique(labels)    \n    n_clusters_ = len(labels_unique)\n    \n    return np.reshape(labels, (*input_img.shape[:2], 1))\n\n\ndef get_image_data(image_id, image_type, **kwargs):\n    if 'mask' in image_type:\n        img = _get_image_data_pil(image_id, image_type, **kwargs)\n    else:\n        img = _get_image_data_opencv(image_id, image_type, **kwargs)\n    return img\n\ndef _get_image_data_opencv(image_id, image_type, **kwargs):\n    fname = get_filename(image_id, image_type)\n    img = cv2.imread(fname)\n    assert img is not None, \"Failed to read image : %s, %s\" % (image_id, image_type)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if True:\n        img = cv2.resize(img, image_size)\n        # Add salt-and-pepper noise to the image.\n#         img = random_noise(img, mode='s&p',amount=0.2)\n    return img/np.max(img)\n\n\ndef _get_image_data_pil(image_id, image_type, return_exif_md=False, return_shape_only=False):\n    fname = get_filename(image_id, image_type)\n    try:\n        img_pil = Image.open(fname)\n    except Exception as e:\n        assert False, \"Failed to read image : %s, %s. Error message: %s\" % (image_id, image_type, e)\n\n    if return_shape_only:\n        return img_pil.size[::-1] + (len(img_pil.getbands()),)\n    if True:\n#         img_pil = img_pil.resize((388, 388))\n        img_pil = img_pil.resize(image_size)\n    img = np.asarray(img_pil)\n    assert isinstance(img, np.ndarray), \"Open image is not an ndarray. Image id/type : %s, %s\" % (image_id, image_type)\n    if not return_exif_md:\n        return img\n    else:\n        return img, img_pil._getexif()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\n\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\n\nimport tensorflow as tf\n\nimport numpy as np\nimport pylab as plt","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, batch_size=32, dim=(32,32,32), n_channels=1, \n                 shuffle=True, image_type='Train', mask_image_type='Train_mask'):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        self.image_type = image_type\n        self.mask_image_type = mask_image_type\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        # cv2 recevrses height and width\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, frames, *dim, n_channels)\n        # Initialization\n        frames = 17  # 1-16\n        batch_size = len(list_IDs_temp)\n    \n        X = np.empty((batch_size, frames - 1, *image_size, 3), dtype=np.float)\n        y = np.empty((batch_size, frames - 1, *image_size), dtype=np.float)\n\n        # Generate data\n        for b, ID in enumerate(list_IDs_temp):\n            for i in range(1, frames):\n                if i < 10:\n                    ext = '0' + str(i)\n                else:\n                    ext = str(i)\n                img, mask = get_image_data(ID + ext, self.image_type), get_image_data(ID + ext, self.mask_image_type)\n                img, mask = random_augmentation(img, mask)\n                X[b,i - 1,:,:,:] = img\n                y[b,i - 1,:,:] = mask\n                \n#                 X[b,i - 1,:,:,:] = get_image_data(ID + ext, 'Train')\n#                 y[b,i - 1,:,:] = get_image_data(ID + ext, 'Train_mask')\n\n        return X, y","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nparams = {'dim': image_size,\n          'batch_size': 1,\n          'n_channels': 3,\n          'shuffle': True}\n\n# Generators\ntraining_generator = DataGenerator(train_ids, **params)\nvalidation_generator = DataGenerator(test_ids, **params)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convLSTM(input_img):\n    convLSTM1 = ConvLSTM2D(filters=128, kernel_size=(3, 3),\n                   padding='same', data_format='channels_last', return_sequences=True, \n                           activation='relu')(input_img)\n    \n    convtd1 = TimeDistributed((Conv2D(128, kernel_size=3, padding = \"same\", activation = 'relu')))(convLSTM1)\n    \n    mptd1 = TimeDistributed(MaxPooling2D((2, 2), (2, 2)))(convtd1)\n    \n    convLSTM2 = ConvLSTM2D(filters=256, kernel_size=(3, 3),\n                   padding='same', data_format='channels_last', return_sequences=True, \n                           activation='relu')(mptd1)\n    \n    convtd2 = TimeDistributed((Conv2D(256, kernel_size=3, padding = \"same\", activation = 'relu')))(convLSTM2)\n    \n    mptd2 = TimeDistributed(MaxPooling2D((2, 2), (2, 2)))(convtd2)\n    \n    convtd3 = TimeDistributed(Conv2D(512, kernel_size=3, activation = 'relu', \n                                       padding = 'same', kernel_initializer = 'he_normal') )(mptd2)\n    convtd4 = TimeDistributed(Conv2D(512, kernel_size=3, activation = 'relu', \n                                       padding = 'same', kernel_initializer = 'he_normal') )(convtd3)\n    \n    \n    upconvtd1 = TimeDistributed(Conv2DTranspose(256, kernel_size=2, strides=2, \n                                                padding='same',kernel_initializer = 'he_normal'))(convtd4)\n    \n    # concatenate convtd4\n    merge1 = concatenate([convtd2, upconvtd1]) # 512 kernel\n    \n    convtd5 = TimeDistributed(Conv2D(256, kernel_size=3, activation = 'relu', \n                                       padding = 'same', kernel_initializer = 'he_normal') )(merge1)\n    convtd6 = TimeDistributed(Conv2D(256, kernel_size=3, activation = 'relu', \n                                       padding = 'same', kernel_initializer = 'he_normal') )(convtd5)\n    \n    upconvtd2 = TimeDistributed(Conv2DTranspose(128, kernel_size=2, strides=2, \n                                        padding='same',kernel_initializer = 'he_normal'))(convtd6)\n    \n    # concatenate convtd2\n    merge2 = concatenate([convtd1, upconvtd2])\n    \n    convtd7 = TimeDistributed(Conv2D(128, kernel_size=3, activation = 'relu', \n                                       padding = 'same', kernel_initializer = 'he_normal') )(merge2)\n    convtd8 = TimeDistributed(Conv2D(128, kernel_size=3, activation = 'relu', \n                                       padding = 'same', kernel_initializer = 'he_normal') )(convtd7)\n    \n    convtd9 = TimeDistributed(Conv2D(1, kernel_size=3, activation = 'sigmoid', \n                                       padding = 'same', kernel_initializer = 'he_normal') )(convtd8)\n    \n    return convtd9","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_img = Input(shape = (16, *params['dim'], params['n_channels']))\n\ndef dice_loss(y_true, y_pred):\n    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n\n    return (numerator + 1) / (denominator + 1)\n\nautoencoder = Model(input_img, convLSTM(input_img))\nautoencoder.compile(loss=keras.losses.binary_crossentropy, metrics=[dice_loss, keras.losses.binary_crossentropy], optimizer='adam')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.summary()","execution_count":19,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 16, 128, 128 0                                            \n__________________________________________________________________________________________________\nconv_lst_m2d (ConvLSTM2D)       (None, 16, 128, 128, 604160      input_1[0][0]                    \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, 16, 128, 128, 147584      conv_lst_m2d[0][0]               \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 16, 64, 64, 1 0           time_distributed[0][0]           \n__________________________________________________________________________________________________\nconv_lst_m2d_1 (ConvLSTM2D)     (None, 16, 64, 64, 2 3539968     time_distributed_1[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_2 (TimeDistrib (None, 16, 64, 64, 2 590080      conv_lst_m2d_1[0][0]             \n__________________________________________________________________________________________________\ntime_distributed_3 (TimeDistrib (None, 16, 32, 32, 2 0           time_distributed_2[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_4 (TimeDistrib (None, 16, 32, 32, 5 1180160     time_distributed_3[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_5 (TimeDistrib (None, 16, 32, 32, 5 2359808     time_distributed_4[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_6 (TimeDistrib (None, 16, 64, 64, 2 524544      time_distributed_5[0][0]         \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 16, 64, 64, 5 0           time_distributed_2[0][0]         \n                                                                 time_distributed_6[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_7 (TimeDistrib (None, 16, 64, 64, 2 1179904     concatenate[0][0]                \n__________________________________________________________________________________________________\ntime_distributed_8 (TimeDistrib (None, 16, 64, 64, 2 590080      time_distributed_7[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_9 (TimeDistrib (None, 16, 128, 128, 131200      time_distributed_8[0][0]         \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 16, 128, 128, 0           time_distributed[0][0]           \n                                                                 time_distributed_9[0][0]         \n__________________________________________________________________________________________________\ntime_distributed_10 (TimeDistri (None, 16, 128, 128, 295040      concatenate_1[0][0]              \n__________________________________________________________________________________________________\ntime_distributed_11 (TimeDistri (None, 16, 128, 128, 147584      time_distributed_10[0][0]        \n__________________________________________________________________________________________________\ntime_distributed_12 (TimeDistri (None, 16, 128, 128, 1153        time_distributed_11[0][0]        \n==================================================================================================\nTotal params: 11,291,265\nTrainable params: 11,291,265\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.fit_generator(generator=training_generator,\n                    validation_data=validation_generator,\n                    use_multiprocessing=True,\n                    workers=1, epochs=10)","execution_count":20,"outputs":[{"output_type":"stream","text":"Train for 250 steps, validate for 68 steps\nEpoch 1/10\n250/250 [==============================] - 226s 903ms/step - loss: 0.3512 - dice_loss: 0.3865 - binary_crossentropy: 0.3512 - val_loss: 0.1754 - val_dice_loss: 0.5815 - val_binary_crossentropy: 0.1754\nEpoch 2/10\n250/250 [==============================] - 210s 842ms/step - loss: 0.1331 - dice_loss: 0.7505 - binary_crossentropy: 0.1331 - val_loss: 0.0917 - val_dice_loss: 0.8582 - val_binary_crossentropy: 0.0917\nEpoch 3/10\n250/250 [==============================] - 207s 830ms/step - loss: 0.0655 - dice_loss: 0.8850 - binary_crossentropy: 0.0655 - val_loss: 0.0496 - val_dice_loss: 0.9092 - val_binary_crossentropy: 0.0496\nEpoch 4/10\n250/250 [==============================] - 207s 827ms/step - loss: 0.0459 - dice_loss: 0.9206 - binary_crossentropy: 0.0459 - val_loss: 0.0426 - val_dice_loss: 0.9316 - val_binary_crossentropy: 0.0426\nEpoch 5/10\n250/250 [==============================] - 207s 827ms/step - loss: 0.0323 - dice_loss: 0.9458 - binary_crossentropy: 0.0323 - val_loss: 0.0270 - val_dice_loss: 0.9552 - val_binary_crossentropy: 0.0270\nEpoch 6/10\n250/250 [==============================] - 208s 834ms/step - loss: 0.0265 - dice_loss: 0.9572 - binary_crossentropy: 0.0265 - val_loss: 0.0314 - val_dice_loss: 0.9607 - val_binary_crossentropy: 0.0314\nEpoch 7/10\n250/250 [==============================] - 207s 828ms/step - loss: 0.0225 - dice_loss: 0.9637 - binary_crossentropy: 0.0225 - val_loss: 0.0218 - val_dice_loss: 0.9607 - val_binary_crossentropy: 0.0218\nEpoch 8/10\n250/250 [==============================] - 208s 834ms/step - loss: 0.0198 - dice_loss: 0.9677 - binary_crossentropy: 0.0198 - val_loss: 0.0208 - val_dice_loss: 0.9653 - val_binary_crossentropy: 0.0208\nEpoch 9/10\n250/250 [==============================] - 208s 833ms/step - loss: 0.0178 - dice_loss: 0.9708 - binary_crossentropy: 0.0178 - val_loss: 0.0164 - val_dice_loss: 0.9732 - val_binary_crossentropy: 0.0164\nEpoch 10/10\n250/250 [==============================] - 207s 829ms/step - loss: 0.0160 - dice_loss: 0.9728 - binary_crossentropy: 0.0160 - val_loss: 0.0164 - val_dice_loss: 0.9739 - val_binary_crossentropy: 0.0164\n","name":"stdout"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fd1b8013940>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs = validation_generator.__getitem__(0)\n\ntraining_images = imgs[0]\ntesting_images = imgs[1]","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = autoencoder.predict(training_images)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(pred[0][15].reshape(128,128))","execution_count":27,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index 16 is out of bounds for axis 0 with size 16","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-e47bb7e138e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for axis 0 with size 16"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}